%%
%% This is file `template-8s.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% template.raw  (with options: `8s')
%% 
%% Template for the LaTeX class aipproc.
%% 
%% (C) 1998,2000,2001 American Institute of Physics and Frank Mittelbach
%% All rights reserved
%% 
%%
%% $Id: template.raw,v 1.12 2005/07/06 19:22:14 frank Exp $
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please remove the next line of code if you
%% are satisfied that your installation is
%% complete and working.
%%
%% It is only there to help you in detecting
%% potential problems.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{aipcheck}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SELECT THE LAYOUT
%%
%% The class supports further options.
%% See aipguide.pdf for details.
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  ,final            % use final for the camera ready runs
%%    ,draft            % use draft while you are working on the paper
%%    ,numberedheadings % uncomment this option for numbered sections
%%  ,                 % add further options here if necessary
  ]
  {aipproc}

\layoutstyle{8x11single}
\usepackage{amsmath}
\usepackage{dsfont}

\newcommand{\pars}{\boldsymbol{\theta}}
\newcommand{\data}{\mathbf{x}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FRONTMATTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Computing astrophysical inferences with diffusive nested sampling}

\classification{<Replace this text with PACS numbers; choose from this list:
                \texttt{http://www.aip..org/pacs/index.html}>}
\keywords      {Astrophysics -- Bayesian Inference -- Nested Sampling}

\author{Brendon J. Brewer}{
  address={Department of Statistics, The University of Auckland\\
http://www.stat.auckland.ac.nz/\~{ }brewer/}
}


\begin{abstract}
Many standard problems in astronomy are best understood as inference problems.
In principle at least,
we should calculate the posterior distribution over a suitable hypothesis space,
rather than inventing ad-hoc procedures. I will describe two recent applications
of this idea: i) inferring the properties of active galactic nuclei (AGN)
from reverberation mapping data, and ii) producing catalogs (lists of objects
and their properties) from images of
crowded stellar fields. The first application allows us to measure the masses
of black holes and to infer the dynamical state of the matter surrounding them,
making the most of the large amount of telescope time required. The second
application has the potential to enable the study of faint objects that are not
even ``detected'' by standard algorithms.
To implement these models we used the Diffusive Nested Sampling algorithm,
which is a variant of Nested Sampling that was invented at MaxEnt 2009. The
algorithm naturally allows for unknown numbers of parameters since
``reversible jump'' MCMC can be used inside of it. However, there are
computational challenges involved when the posterior distribution is highly
compressed with respect to the prior distribution.
\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MAINMATTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The major goal of astronomy, as with most sciences, is to understand the
universe as well as we can while recognising that we will never have all of
the relevant information that we might want. Probability theory is the
appropriate framework for describing and quantifying uncertainty
\citep{cox, jaynes}. In recent years, Bayesian inference has (rightfully)
become the dominant framework for analysing data and quantifying the
plausibility of hypotheses about the universe, and has displaced
more ad-hoc methodologies in many subfields of
astronomy \citep[e.g.][]{examples}.

Bayesian inference is usually performed by
constructing a set of possible answers to the question of
interest, that is, the ``hypothesis space'', indexed by parameters $\pars$.
A state of prior knowledge about the parameters is described by assigning a
prior distribution $p(\pars)$. Some relevant data $\data$ is about to become
known, and prior knowledge about a connection between $\theta$ and $\data$ is
expressed by a choice of ``sampling distribution'' $p(\data | \theta)$, which
is really a family of probability distributions, one for each possible value of
$\theta$.

By the product rule, the joint (prior) distribution is
\begin{eqnarray}
p(\pars, \data) = p(\pars) p(\data | \pars) = p(\data)p(\pars | \data)
\end{eqnarray}
When the
data $\data$ becomes known, our state of knowledge about $\pars$ changes
from the marginal $p(\pars)$ to the conditional distribution
$p(\pars | \data_{\rm actual})$
that corresponds to the observed data. The distribution
$p(\pars | \data_{\rm actual})$
called the ``posterior''.
Incidentally, Bayesian inference can be understood as a special case of
MaxEnt updating, starting from the joint prior $p(\pars, \data)$ and applying
the constraint $P(\data = \data_{\rm actual}) = 1$ \citep{caticha}. Note that
this is not a ``derivation'' of Bayesian updating as the rules of probability
already imply a commitment to update from $p(\theta)$ to $p(\theta | \data)$
once $\data$ becomes known.

Computing the consequences of a posterior distribution can be a challenging
task. Usually we want to marginalise out nuisance parameters, to compute
particular posterior probabilities such as $P(\theta > 1.5)$, or to summarise
the posterior distribution for easy communication with others.
Markov Chain Monte Carlo (MCMC) algorithms provide a general set of tools
for achieving these goals. However, many standard MCMC algorithms are not
capable of exploring complicated probability distributions in a reasonable time.
Problems often occur due to multimodality, strong dependencies, and phase
transitions.

\section{Nested sampling}
The key idea of Nested Sampling \citep{skilling} is that Bayesian computation
should start from the prior $\pi(\pars)$ and move through a sequence of
distributions that successively compress the prior volume by a constant factor
that is (approximately) known.

\subsection{MCMC inside nested sampling}
The main challenge in Nested Sampling is to generate new points from the
constrained prior distribution $\propto \pi(\pars)\mathds{1}(L(\pars) > L^*)$.
Two popular strategies are in use. The first is to use the remaining ``live
points'', which are known to be above the likelihood threshold $L^*$, to build
an approximation to the constrained prior, which can then be used to generate
a new point via rejection sampling \citep{multinest}.

The second common strategy is to use MCMC. Since the remaining live points are
known to exceed the likelihood threshold, they can be used to initialise an
MCMC run. This strategy is very simple but has known problems. For example,
suppose the likelihood function has two separate modes, and it impossible for
MCMC to mix between the two modes. Suppose also that NS is being run with two
points, and there is one point in each mode. The worst point will be discarded
(and its information saved elsewhere), and the new point will be generated by
copying the other point and running MCMC. However, even though the two modes
are completely equivalent, the copying operation will remove one of the modes,
and it will never return.





\section{Diffusive nested sampling}
Diffusive Nested Sampling (DNS) is an alternative to the use of MCMC inside of
standard Nested Sampling. DNS is essentially just the Metropolis algorithm.
However, instead of the
target distribution being the posterior $p(\pars|\data)$, the algorithm
constructs and then samples a ``mixture of constrained priors'':

\begin{eqnarray}
p_{\rm DNS}(\theta) &=& \frac{1}{n}\sum_{i=1}^n
\frac{p(\theta)\mathds{1}\left[L(\theta) > L^*_i\right]}{X_i}
\end{eqnarray}

The zeroth component of this mixture is just the prior $p(\pars)$. The first
mixture component is the prior, but restricted to the domain where the
likelihood is greater than a threshold $L^*_1$. The second mixture component
is the prior, but restricted to the domain where the likelihood is above
$L^*_2$. The likelihood thresholds $\{ L^*_i \}$ are chosen so each
successive constrained prior occupies approximately $\exp(-1)$ times as much
prior mass (``volume'') as the previous constrained prior. This is strongly
related to the algorithm known as ``simulated tempering''
\citep{simulated_tempering}, but uses NS's
sequence of intermediate distributions instead of an annealed sequence.

\section{Probabilistic Catalogs}
The hypothesis space is the set of possible answers to the question
``how many objects are there, and what are their properties (positions, fluxes,
etc)''.


%\section{Reverberation Mapping}
%Many objects in the universe are too distant to resolve, so we cannot simply
%``see'' what is going on. Rather we must infer what is going on. Active Galactic
%Nuclei (AGN) are a good example.

\subsection{Models with known oversimplifications}
Computing the posterior $p(\pars | \data)$ would yield overconfident results
since the assumed model is known to be wrong. However certain parameters still
``exist'' (such as the black hole mass) and we want our posterior inferences
to be realistic.

We have been using the following subjective procedure. Later we give two
interpretations of this procedure which may lead the way to less subjective
approaches.
Using DNS we explore the mixture of constrained priors.

where $X_i \approx e^{-i}$ by construction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BACKMATTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theacknowledgments}
Acknowledgements.
\end{theacknowledgments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The bibliography can be prepared using the BibTeX program or
%% manually.
%%
%% The code below assumes that BibTeX is used.  If the bibliography is
%% produced without BibTeX comment out the following lines and see the
%% aipguide.pdf for further information.
%%
%% For your convenience a manually coded example is appended
%% after the \end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% You may have to change the BibTeX style below, depending on your
%% setup or preferences.
%%
%%
%% For The AIP proceedings layouts use either
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{aipproc}   % if natbib is available
%\bibliographystyle{aipprocl} % if natbib is missing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% You probably want to use your own bibtex database here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliography{sample}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Just a reminder that you may have to run bibtex
%%% All of it up to \end{document} can be removed
%%% if you don't like the warning.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\IfFileExists{\jobname.bbl}{}
% {\typeout{}
%  \typeout{******************************************}
%  \typeout{** Please run "bibtex \jobname" to optain}
%  \typeout{** the bibliography and then re-run LaTeX}
%  \typeout{** twice to fix the references!}
%  \typeout{******************************************}
%  \typeout{}
% }

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The following lines show an example how to produce a bibliography
%% without the help of the BibTeX program. This could be used instead
%% of the above.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{9}

\bibitem{Brown2000}
M.~P. Brown,  and K.~Austin, \emph{The New Physique}, Publisher Name,
  Publisher City, 2000, pp. 212--213.

\bibitem{BrownAustin:2000}
M.~P. Brown,  and K.~Austin, \emph{Appl. Phys. Letters} \textbf{85},
  2503--2504 (2000).

\bibitem{Wang}
R.~Wang, ``Title of Chapter,'' in \emph{Classic Physiques}, edited by
  R.~B. Hamil, Publisher Name, Publisher City, 2000, pp. 212--213.

\bibitem{SJ:1999}
C.~D.~Smith and E.~F.~Jones,  ``Load-Cycling in Cubic Press,'' in
  \emph{Shock Compression of Condensed Matter-1999}, edited by M.~D.~F. et~al.,
  AIP Conference Proceedings 505, American Institute of Physics, New York,
  1999, pp. 651--654.

\end{thebibliography}

\endinput
%%
%% End of file `template-8s.tex'.
