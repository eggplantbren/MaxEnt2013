%%
%% This is file `template-8s.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% template.raw  (with options: `8s')
%% 
%% Template for the LaTeX class aipproc.
%% 
%% (C) 1998,2000,2001 American Institute of Physics and Frank Mittelbach
%% All rights reserved
%% 
%%
%% $Id: template.raw,v 1.12 2005/07/06 19:22:14 frank Exp $
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please remove the next line of code if you
%% are satisfied that your installation is
%% complete and working.
%%
%% It is only there to help you in detecting
%% potential problems.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{aipcheck}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% SELECT THE LAYOUT
%%
%% The class supports further options.
%% See aipguide.pdf for details.
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  ,final            % use final for the camera ready runs
%%    ,draft            % use draft while you are working on the paper
%%    ,numberedheadings % uncomment this option for numbered sections
%%  ,                 % add further options here if necessary
  ]
  {aipproc}

\layoutstyle{8x11single}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{natbib}

\newcommand{\pars}{\boldsymbol{\theta}}
\newcommand{\data}{\mathbf{x}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% FRONTMATTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Computing astrophysical inferences with diffusive nested sampling}

\classification{<Replace this text with PACS numbers; choose from this list:
                \texttt{http://www.aip..org/pacs/index.html}>}
\keywords      {Astrophysics -- Bayesian Inference -- Nested Sampling}

\author{Brendon J. Brewer}{
  address={Department of Statistics, The University of Auckland\\
http://www.stat.auckland.ac.nz/\~{ }brewer/}
}


\begin{abstract}
Many standard problems in astronomy are best understood as inference problems.
In principle at least,
we should calculate the posterior distribution over a suitable hypothesis space,
rather than inventing ad-hoc procedures. I will describe a recent application
of this idea to the challenging astronomical problem of
producing catalogs (lists of objects
and their properties) from images of
crowded stellar fields. This approach
has the potential to enable the study of faint objects that are not
even ``detected'' by standard algorithms.
To implement the model we used the Diffusive Nested Sampling algorithm,
which is a variant of Nested Sampling that was invented at MaxEnt 2009. The
algorithm naturally allows for unknown numbers of parameters since
``reversible jump'' MCMC can be used inside of it. However, there are
computational challenges involved when the posterior distribution is highly
compressed with respect to the prior distribution.
\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MAINMATTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The major goal of astronomy, as with most sciences, is to understand the
universe as well as we can while recognising that we will never have all of
the relevant information that we might want. Probability theory is the
appropriate framework for describing and quantifying uncertainty
\citep{cox, jaynes}. In recent years, Bayesian inference has (rightfully)
become the dominant framework for analysing data and quantifying the
plausibility of hypotheses about the universe, and has displaced
more ad-hoc methodologies in many subfields of
astronomy \citep[e.g.][]{2014MNRAS.438..768S, 2014MNRAS.437.3540F,
2014MNRAS.437.3004L}.

Bayesian inference is usually performed by
constructing a set of possible answers to the question of
interest, that is, the ``hypothesis space'', indexed by parameters $\pars$.
A state of prior knowledge about the parameters is described by assigning a
prior distribution $p(\pars)$. Some relevant data $\data$ is about to become
known, and prior knowledge about a connection between $\theta$ and $\data$ is
expressed by a choice of ``sampling distribution'' $p(\data | \theta)$, which
is really a family of probability distributions, one for each possible value of
$\theta$. By the product rule, the joint (prior) distribution is
\begin{eqnarray}
p(\pars, \data) = p(\pars) p(\data | \pars) = p(\data)p(\pars | \data)
\end{eqnarray}
When the
data $\data$ becomes known, our state of knowledge about $\pars$ changes
from the marginal distribution $p(\pars)$ to the conditional distribution
$p(\pars | \data_{\rm actual})$
that corresponds to the observed data. The distribution
$p(\pars | \data_{\rm actual})$ is
called the ``posterior''.
Incidentally, Bayesian inference can be understood as a special case of
MaxEnt updating, starting from the joint prior $p(\pars, \data)$ and applying
the constraint $P(\data = \data_{\rm actual}) = 1$ \citep{caticha}. Note that
this is not a ``derivation'' of Bayesian updating as the rules of probability
already imply a commitment to update from $p(\theta)$ to $p(\theta | \data)$
once $\data$ becomes known. For convenience, once the data set has been
established and attention turns to computation, the prior is denoted by
$\pi(\theta)$ and the likelihood function is $L(\theta)$.

Computing the consequences of a posterior distribution can be a challenging
task. Usually we want to marginalise away nuisance parameters, to compute
particular posterior probabilities such as $P(\theta > 1.5)$, or to summarise
the posterior distribution for easy communication with others, for example by
calculating credible intervals and stating that $a \leq \theta b$ with 95\%
probability.
Markov Chain Monte Carlo (MCMC) algorithms provide a general set of tools
for achieving these goals. However, many standard MCMC algorithms are not
capable of exploring complicated probability distributions in a reasonable time.
Problems often occur due to multiple modes, strong dependencies
between parameters, and (less widely recognised, but also less commonly) phase
transitions. Many different solutions have been suggested for overcoming these
challenges. This paper focuses on Nested Sampling \citep{skilling}.

\section{Nested sampling}
The key idea of Nested Sampling \citep{skilling} is that Bayesian computation
should not solely be aimed at obtaining samples from the posterior distribution.
By beginning from the prior $\pi(\pars)$ and moving through a sequence of
distributions that successively compress the prior volume by a constant factor
that is (approximately) known, Nested Sampling can calculate posterior samples
as well as the marginal likelihood or evidence
$Z = \int p(\pars)p(\data = \data_{\rm actual} | \pars) \, d\pars$.


\subsection{MCMC inside nested sampling}
The main challenge in Nested Sampling is to generate new points from the
constrained prior distribution $\propto \pi(\pars)\mathds{1}(L(\pars) > L^*)$.
Two popular strategies are in use. The first is to use the remaining ``live
points'', which are known to be above the likelihood threshold $L^*$, to build
an approximation to the constrained prior, which can then be used to generate
a new point via rejection sampling, as implemented in the
{\tt MultiNest} program \citep{multinest}.

The second common strategy is to use MCMC. Since all of the Nested Sampling
points are
known to exceed the likelihood threshold (which is defined using the lowest
point), they can be used to initialise an
MCMC run. This strategy is very simple to implement but can have
serious problems. For example,
suppose the likelihood function has two separate modes, and it impossible for
MCMC to mix between the two modes. Suppose also that NS is being run with two
points, and there is one point in each mode. The worst point will be discarded
(and its information saved elsewhere), and the new point will be generated by
copying the other point and running MCMC. However, even though the two modes
are completely equivalent, the copying operation will remove one of the modes,
and it will never return.

\section{Diffusive nested sampling}
Diffusive Nested Sampling (DNS) is an alternative to the use of MCMC inside of
standard Nested Sampling. DNS is essentially just the Metropolis algorithm.
However, instead of the
target distribution being the posterior $p(\pars|\data)$, the algorithm
constructs and then samples a ``mixture of constrained priors'':

\begin{eqnarray}
p_{\rm DNS}(\theta) &=& \frac{1}{n}\sum_{i=1}^n
\frac{p(\theta)\mathds{1}\left[L(\theta) > L^*_i\right]}{X_i}
\end{eqnarray}

The zeroth component of this mixture is just the prior $p(\pars)$. The first
mixture component is the prior, but restricted to the domain where the
likelihood is greater than a threshold $L^*_1$. The second mixture component
is the prior, but restricted to the domain where the likelihood is above
$L^*_2$. The likelihood thresholds $\{ L^*_i \}$ are chosen so each
successive constrained prior occupies approximately $\exp(-1)$ times as much
prior mass (``volume'') as the previous constrained prior. This is strongly
related to the algorithm known as ``simulated tempering''
\citep{simulated_tempering}, but uses NS's
sequence of intermediate distributions instead of an annealed sequence
based on $\pi(\theta)L(\theta)^\beta$. Retaining ``old'' distributions in the
mixture gives the particles a chance to explore more freely, perhaps escaping
one mode in the distribution and moving to another.

A key advantage of DNS over standard MCMC-based Nested Sampling is the performance
on multimodal distributions. With DNS, if two particles are in two separate
but identical modes, there is no need for any deleting or copying of particles.
The two particles will simply continue exploring their own modes. A particle is
only deleted if it is unable to keep up with the bulk of the other particles.
This way, the diversity created by having multiple starting points is maintained
for as long as possible. A multi-threaded, free software C++ implementation of
DNS, called {\tt DNest3},
is available online at {\tt http://github.com/eggplantbren/DNest3}.

\section{Probabilistic Catalogs}
Many questions in astronomy can be classified as ``source detection'' problems.
Telescopes (whether optical, radio, or whatever) obtain some data, and we would
like to use this data to infer the presence and properties of the objects in
that part of the sky. Several different techniques and software packages have
been developed for carrying this out \citep[e.g.][]{sextractor, dolphot}.
However, this process
can also be described as a Bayesian inference problem \citep{starfield}.
The hypothesis space is the set of possible answers to the question
``how many objects are there, and what are their properties (positions, fluxes,
etc)''. This problem has been addressed from a Bayesian point of view by several
authors \citep[e.g.][]{2003MNRAS.338..765H}.

Recently we investigated a Bayesian approach for finding objects in ``crowded
fields'', small patches of the sky where many stars are visible in an image.
Each star would appear as a point source in the image, except for the effects
of blurring (caused by either diffraction as the light enters the telescope,
or by atmospheric fluctuations), and the images are also typically noisy.
Since the number of stars $N$ is itself unknown, we use reversible jump
MCMC {\it within} DNS to infer $N$.

For the ``easy'' simulated data set containing 100 stars, the information
$H$ was approximately 500 nats, whereas for the ``hard'' dataset the information
was $\sim$2600 nats. In both cases, the upward part of the DNS run was
straightforward. However, in the ``hard'' case the diffusion phase of DNS
is very slow. Unfortunately, I am presently unaware of any Nested Sampling
method that only moves upwards in likelihood but is able to correct for the
use of imperfect MCMC. Variations of annealing or tempering such as
``annealed importance sampling'' and ``linked importance sampling'' \citep{ais, lis}
have both these properties, suggesting this may be possible with NS as well.





\subsection{Models with known oversimplifications}
Computing the posterior $p(\pars | \data)$ would yield overconfident results
since the assumed model is known to be wrong. However certain parameters still
``exist'' (such as the black hole mass) and we want our posterior inferences
to be realistic.

We have been using the following subjective procedure. Later we give two
interpretations of this procedure which may lead the way to less subjective
approaches.
Using DNS we explore the mixture of constrained priors.

where $X_i \approx e^{-i}$ by construction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% BACKMATTER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theacknowledgments}
It is a pleasure to thank David W. Hogg (NYU), Daniel Foreman-Mackey (NYU),
Fengji Hou (NYU), Anna Pancoast (UCSB), Benjamin Pope (Oxford), and
John Skilling (MaxEnt Data Consultants) for valuable discussions.
\end{theacknowledgments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The bibliography can be prepared using the BibTeX program or
%% manually.
%%
%% The code below assumes that BibTeX is used.  If the bibliography is
%% produced without BibTeX comment out the following lines and see the
%% aipguide.pdf for further information.
%%
%% For your convenience a manually coded example is appended
%% after the \end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% You may have to change the BibTeX style below, depending on your
%% setup or preferences.
%%
%%
%% For The AIP proceedings layouts use either
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{aipproc}   % if natbib is available
%\bibliographystyle{aipprocl} % if natbib is missing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% You probably want to use your own bibtex database here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliography{sample}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Just a reminder that you may have to run bibtex
%%% All of it up to \end{document} can be removed
%%% if you don't like the warning.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\IfFileExists{\jobname.bbl}{}
% {\typeout{}
%  \typeout{******************************************}
%  \typeout{** Please run "bibtex \jobname" to optain}
%  \typeout{** the bibliography and then re-run LaTeX}
%  \typeout{** twice to fix the references!}
%  \typeout{******************************************}
%  \typeout{}
% }

\begin{thebibliography}{9}
\bibitem[Bertin and
Arnouts(1996)]{sextractor} Bertin, E., Arnouts, S.\ 1996.\ SExtractor: Software for source extraction.\ Astronomy and Astrophysics Supplement Series 117, 393-404.

\bibitem[Brewer et al.(2013)]{starfield} Brewer, B.~J., 
Foreman-Mackey, D., Hogg, D.~W.\ 2013.\ Probabilistic Catalogs for Crowded 
Stellar Fields.\ The Astronomical Journal 146, 7. 

\bibitem[Brewer et al.(2011)]{dnest}
Brewer, B.~J., P\'{a}rtay, L.~B., Cs\'{a}nyi, G.\ 2011.\
Diffusive Nested Sampling.\ Statistics and Computing, 2011, 21, 4, 649-656.

\bibitem[Caticha(2008)]{caticha} Caticha, A.\ 2008.\ Lectures 
on Probability, Entropy, and Statistical Physics.\ ArXiv e-prints 
arXiv:0808.0012. 

\bibitem[Cox(1946)]{cox} Cox, R.~T.\ 1946.\ Probability, 
Frequency and Reasonable Expectation.\ American Journal of Physics 14, 
1-13. 

\bibitem[Dolphin(2000)]{dolphot} Dolphin, A.~E.\ 2000, Publications of the
Astronomical Society of the Pacific, 112, 1383 

\bibitem[Feroz et al.(2009)]{multinest} Feroz, F., Hobson, M.~P., 
Bridges, M.\ 2009.\ MULTINEST: an efficient and robust Bayesian inference 
tool for cosmology and particle physics.\ Monthly Notices of the Royal 
Astronomical Society 398, 1601-1614. 

\bibitem[Feroz and Hobson(2014)]{2014MNRAS.437.3540F} Feroz, F., Hobson, 
M.~P.\ 2014.\ Bayesian analysis of radial velocity data of GJ667C with 
correlated noise: evidence for only two planets.\ Monthly Notices of the 
Royal Astronomical Society 437, 3540-3549. 

\bibitem[Hobson 
\& McLachlan(2003)]{2003MNRAS.338..765H} Hobson, M.~P., \& McLachlan, C.\ 2003, Monthly Notices of the Royal Astronomical Society, 338, 765 

\bibitem[Jaynes and Bretthorst(2003)]{jaynes} Jaynes, E.~T., 
Bretthorst, G.~L.\ 2003.\ Probability Theory.\ Probability Theory, by 
E.~T.~Jaynes and Edited by G.~Larry Bretthorst, pp.~758.~ISBN 
0521592712.~Cambridge, UK: Cambridge University Press, June 2003.\ . 

\bibitem[Lentati et al.(2014)]{2014MNRAS.437.3004L} Lentati, L., Alexander, 
P., Hobson, M.~P., Feroz, F., van Haasteren, R., Lee, K.~J., Shannon, 
R.~M.\ 2014.\ TEMPONEST: a Bayesian approach to pulsar timing analysis.\ 
Monthly Notices of the Royal Astronomical Society 437, 3004-3023. 

\bibitem[Marinari and Parisi(1992)]{simulated_tempering} Marinari, E., 
Parisi, G.\ 1992.\ Simulated tempering: a new Monte Carlo scheme.\ EPL 
(Europhysics Letters) 19, 451. 

\bibitem[Neal(1998)]{ais} Neal, R.~M.\ 1998.\ Annealed 
Importance Sampling.\ ArXiv Physics e-prints arXiv:physics/9803008. 

\bibitem[Neal(2005)]{lis} Neal, R.~M.\ 2005.\ Estimating 
Ratios of Normalizing Constants Using Linked Importance Sampling.\ ArXiv 
Mathematics e-prints arXiv:math/0511216. 

\bibitem[\protect\citeauthoryear{Skilling}{2006}]{skilling} Skilling, J., 2006, Nested Sampling for General Bayesian Computation, Bayesian Analysis 4, pp. 833-860.

\bibitem[Sutter et al.(2014)]{2014MNRAS.438..768S} Sutter, P.~M., Wandelt, 
B.~D., McEwen, J.~D., Bunn, E.~F., Karakci, A., Korotkov, A., Timbie, P., 
Tucker, G.~S., Zhang, L.\ 2014.\ Probabilistic image reconstruction for 
radio interferometers.\ Monthly Notices of the Royal Astronomical Society 
438, 768-778. 

\end{thebibliography}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The following lines show an example how to produce a bibliography
%% without the help of the BibTeX program. This could be used instead
%% of the above.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\endinput
%%
%% End of file `template-8s.tex'.
